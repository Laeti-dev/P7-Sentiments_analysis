{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e678529e",
   "metadata": {},
   "source": [
    "# PyTorch on macOS Guide\n",
    "\n",
    "This notebook provides a comprehensive guide for using PyTorch effectively on macOS, with a focus on:\n",
    "\n",
    "1. Setting up PyTorch with hardware acceleration\n",
    "2. Using Metal Performance Shaders (MPS) on Apple Silicon\n",
    "3. Working with BERT models using PyTorch on Mac\n",
    "4. Performance optimization techniques\n",
    "5. Troubleshooting common issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d9f981d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.8.0\n",
      "CUDA available: False\n",
      "MPS available: True\n",
      "System: Darwin arm64\n",
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import platform\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "# Check for MPS (Metal Performance Shaders) on macOS with Apple Silicon\n",
    "mps_available = hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available()\n",
    "print(f\"MPS available: {mps_available}\")\n",
    "print(f\"System: {platform.system()} {platform.machine()}\")\n",
    "\n",
    "# Determine the best available device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if mps_available else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA Device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79720b3d",
   "metadata": {},
   "source": [
    "# Using PyTorch on macOS\n",
    "\n",
    "This notebook demonstrates how to effectively use PyTorch on macOS, including hardware acceleration with Metal Performance Shaders (MPS) on Apple Silicon devices (M1/M2/M3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1addc3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Tensor created on mps:\n",
      "tensor([[0.3890, 0.0666, 0.9399],\n",
      "        [0.2088, 0.2788, 0.2158],\n",
      "        [0.7727, 0.8135, 0.2127],\n",
      "        [0.8628, 0.5630, 0.9082],\n",
      "        [0.1651, 0.7615, 0.7113]], device='mps:0')\n",
      "Matrix multiplication result:\n",
      "tensor([[1.1655, 0.1997],\n",
      "        [0.5208, 0.0950],\n",
      "        [1.1879, 0.2541],\n",
      "        [1.7323, 0.3345],\n",
      "        [1.3454, 0.1998]], device='mps:0')\n",
      "\n",
      "Performance comparison for 1000x1000 matrix multiplication:\n",
      "CPU time: 0.0040 seconds\n",
      "MPS time: 0.0109 seconds\n",
      "Speedup: 0.36x\n"
     ]
    }
   ],
   "source": [
    "# Example: Using MPS for hardware acceleration on macOS\n",
    "\n",
    "# Determine the best available device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Create a sample tensor and move it to the device\n",
    "x = torch.rand(5, 3)\n",
    "x = x.to(device)\n",
    "print(f\"Tensor created on {device}:\\n{x}\")\n",
    "\n",
    "# Simple matrix multiplication example\n",
    "y = torch.rand(3, 2).to(device)\n",
    "z = torch.matmul(x, y)\n",
    "print(f\"Matrix multiplication result:\\n{z}\")\n",
    "\n",
    "# Compare performance between CPU and device (if MPS/CUDA is available)\n",
    "import time\n",
    "\n",
    "# Function to measure performance\n",
    "def measure_performance(device_type, size=1000):\n",
    "    start = time.time()\n",
    "\n",
    "    # Create matrices on the specified device\n",
    "    matrix1 = torch.rand(size, size, device=device_type)\n",
    "    matrix2 = torch.rand(size, size, device=device_type)\n",
    "\n",
    "    # Matrix multiplication\n",
    "    result = torch.matmul(matrix1, matrix2)\n",
    "\n",
    "    # Force computation to complete (important for timing)\n",
    "    result.mean().item()\n",
    "\n",
    "    end = time.time()\n",
    "    return end - start\n",
    "\n",
    "# Only compare if we have hardware acceleration\n",
    "if device != \"cpu\":\n",
    "    cpu_time = measure_performance(\"cpu\", size=1000)\n",
    "    device_time = measure_performance(device, size=1000)\n",
    "    speedup = cpu_time / device_time\n",
    "\n",
    "    print(f\"\\nPerformance comparison for 1000x1000 matrix multiplication:\")\n",
    "    print(f\"CPU time: {cpu_time:.4f} seconds\")\n",
    "    print(f\"{device.upper()} time: {device_time:.4f} seconds\")\n",
    "    print(f\"Speedup: {speedup:.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5fb99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import subprocess\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.amp import autocast, GradScaler\n",
    "from torch.optim import AdamW\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, get_scheduler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import zipfile\n",
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faba3809",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow.models.signature import infer_signature\n",
    "from mlflow import MlflowClient\n",
    "from mlflow.models.signature import ModelSignature\n",
    "from mlflow.types import Schema, ColSpec, TensorSpec\n",
    "from google.colab import userdata\n",
    "import mlflow.pytorch\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# For scikit-learn\n",
    "mlflow.sklearn.autolog()\n",
    "\n",
    "# Configuring MLflow\n",
    "load_dotenv()\n",
    "tracking_uri = os.getenv(\"MLFLOW_TRACKING_URI\")\n",
    "mlflow.set_tracking_uri(tracking_uri)\n",
    "print(f\"MLflow Tracking URI: {tracking_uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35908ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample = pd.read_csv('../data/processed_sample.csv')\n",
    "df_sample.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893bea0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_sample['target'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7316e298",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_test_split' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Split data into training and test sets\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m train_val_texts, test_texts, train_val_labels, test_labels = \u001b[43mtrain_test_split\u001b[49m(\n\u001b[32m      3\u001b[39m         df_sample[\u001b[33m'\u001b[39m\u001b[33mprocessed_text\u001b[39m\u001b[33m'\u001b[39m].values,\n\u001b[32m      4\u001b[39m         df_sample[\u001b[33m'\u001b[39m\u001b[33mtarget\u001b[39m\u001b[33m'\u001b[39m].values,\n\u001b[32m      5\u001b[39m         test_size=\u001b[32m0.2\u001b[39m,\n\u001b[32m      6\u001b[39m         random_state=\u001b[32m42\u001b[39m\n\u001b[32m      7\u001b[39m     )\n\u001b[32m      9\u001b[39m train_texts, val_texts, train_labels, val_labels = train_test_split(\n\u001b[32m     10\u001b[39m     train_val_texts,\n\u001b[32m     11\u001b[39m     train_val_labels,\n\u001b[32m     12\u001b[39m     test_size=\u001b[32m0.2\u001b[39m,\n\u001b[32m     13\u001b[39m     random_state=\u001b[32m42\u001b[39m\n\u001b[32m     14\u001b[39m )\n\u001b[32m     16\u001b[39m data = {\n\u001b[32m     17\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m'\u001b[39m: {\u001b[33m'\u001b[39m\u001b[33mtexts\u001b[39m\u001b[33m'\u001b[39m: train_texts, \u001b[33m'\u001b[39m\u001b[33mlabels\u001b[39m\u001b[33m'\u001b[39m: train_labels},\n\u001b[32m     18\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mval\u001b[39m\u001b[33m'\u001b[39m: {\u001b[33m'\u001b[39m\u001b[33mtexts\u001b[39m\u001b[33m'\u001b[39m: val_texts, \u001b[33m'\u001b[39m\u001b[33mlabels\u001b[39m\u001b[33m'\u001b[39m: val_labels},\n\u001b[32m     19\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mtest\u001b[39m\u001b[33m'\u001b[39m: {\u001b[33m'\u001b[39m\u001b[33mtexts\u001b[39m\u001b[33m'\u001b[39m: test_texts, \u001b[33m'\u001b[39m\u001b[33mlabels\u001b[39m\u001b[33m'\u001b[39m: test_labels}\n\u001b[32m     20\u001b[39m     }\n",
      "\u001b[31mNameError\u001b[39m: name 'train_test_split' is not defined"
     ]
    }
   ],
   "source": [
    "# Split data into training and test sets\n",
    "train_val_texts, test_texts, train_val_labels, test_labels = train_test_split(\n",
    "        df_sample['processed_text'].values,\n",
    "        df_sample['target'].values,\n",
    "        test_size=0.2,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    train_val_texts,\n",
    "    train_val_labels,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "data = {\n",
    "        'train': {'texts': train_texts, 'labels': train_labels},\n",
    "        'val': {'texts': val_texts, 'labels': val_labels},\n",
    "        'test': {'texts': test_texts, 'labels': test_labels}\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62dc2367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize text\n",
    "def tokenize_texts(tokenizer, texts, max_length=128):\n",
    "    return tokenizer(\n",
    "        list(texts),\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors='pt'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cead5820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "def train_model(model, train_loader, val_loader, num_epochs=3, gradient_accumulation_steps=4):\n",
    "    \"\"\"Trains BERT model and returns history.\n",
    "\n",
    "    Args:\n",
    "        model (_type_): _description_\n",
    "        train_loader (_type_): _description_\n",
    "        val_loader (_type_): _description_\n",
    "        num_epochs (int, optional): _description_. Defaults to 3.\n",
    "        gradient_accumulation_steps (int, optional): _description_. Defaults to 4.\n",
    "    \"\"\"\n",
    "    scaler = GradScaler()\n",
    "    optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
    "\n",
    "    total_steps = len(train_loader) * num_epochs // gradient_accumulation_steps\n",
    "    scheduler = get_scheduler(\n",
    "        \"linear\",\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "\n",
    "    history = {'train_loss': [], 'val_loss': [], 'train_accuracy': [], 'val_accuracy': []}\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        total_train_correct = 0\n",
    "        train_preds, train_true = [], []\n",
    "\n",
    "        train_progress_bar = tqdm(train_loader, desc=f\"Training Epoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    for batch_idx, batch in enumerate(train_progress_bar):\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "\n",
    "                with autocast():\n",
    "                    outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                    loss = outputs.loss / gradient_accumulation_steps\n",
    "                    logits = outputs.logits\n",
    "\n",
    "                scaler.scale(loss).backward()\n",
    "\n",
    "                # preds = torch.argmax(logits, dim=1)\n",
    "                # total_train_correct += (preds == labels).sum().item()\n",
    "                # train_preds.extend(preds.cpu().numpy())\n",
    "                # train_true.extend(labels.cpu().numpy())\n",
    "                # total_train_loss += loss.item() * gradient_accumulation_steps\n",
    "\n",
    "                # Update weights every 'gradient_accumulation_steps' batches\n",
    "                if (batch_idx + 1) % gradient_accumulation_steps == 0:\n",
    "                    scaler.unscale_(optimizer)\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()\n",
    "                    optimizer.zero_grad()\n",
    "                    scheduler.step()\n",
    "\n",
    "                train_progress_bar.set_postfix({'loss': loss.item() * gradient_accumulation_steps})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf24dfc",
   "metadata": {},
   "source": [
    "# Using PyTorch with BERT on macOS\n",
    "\n",
    "When working with BERT and other transformer models on macOS, here are some best practices:\n",
    "\n",
    "1. **Use hardware acceleration with MPS** on Apple Silicon Macs (M1/M2/M3)\n",
    "2. **Manage batch sizes** carefully based on your available memory\n",
    "3. **Use mixed precision** when possible to improve performance\n",
    "4. **Monitor memory usage** as transformer models can be memory-intensive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af6fd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Using BERT with MPS on macOS\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load a pre-trained BERT model and tokenizer\n",
    "try:\n",
    "    # model_name = \"finiteautomata/bertweet-base-sentiment-analysis\"\n",
    "    model_name = \"bert-base-uncased\"\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "    model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "    # Move model to device (MPS/CUDA/CPU)\n",
    "    model = model.to(device)\n",
    "    print(f\"Model loaded successfully and moved to {device}\")\n",
    "\n",
    "    documents = df_sample['text']\n",
    "    labels = df_sample['target']\n",
    "\n",
    "    # Tokenize and prepare inputs\n",
    "    def tokenize_data(documents):\n",
    "        return tokenizer(\n",
    "            documents.tolist(),\n",
    "            max_length=128, padding=True, truncation=True, return_tensors='pt'\n",
    "        )\n",
    "    tokens = tokenize_data(documents)\n",
    "\n",
    "    # Convert to numPy arrays for DataLoader\n",
    "    input_ids_np = tokens['input_ids'].numpy()\n",
    "    attention_masks_np = tokens['attention_mask'].numpy()\n",
    "    labels_np = labels.to_numpy()\n",
    "\n",
    "    train_input_ids, val_input_ids, train_labels, val_labels = train_test_split(input_ids_np, labels_np, test_size=0.2, random_state=42)\n",
    "    train_attention_masks, val_attention_masks = train_test_split(attention_masks_np, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Move inputs to device\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    # Inference\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "\n",
    "    # Move predictions back to CPU for processing\n",
    "    predictions = predictions.cpu().numpy()\n",
    "\n",
    "    # Display results\n",
    "    for text, pred in zip(example_texts, predictions):\n",
    "        sentiment = \"Positive\" if pred.argmax() == 2 else \"Neutral\" if pred.argmax() == 1 else \"Negative\"\n",
    "        print(f\"Text: '{text}'\")\n",
    "        print(f\"Sentiment: {sentiment} (Confidence: {pred.max():.4f})\")\n",
    "        print()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error loading or using model: {str(e)}\")\n",
    "    print(\"If you're having issues with the model, try installing additional dependencies:\")\n",
    "    print(\"!pip install transformers[torch] datasets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0efa2884",
   "metadata": {},
   "source": [
    "# PyTorch on macOS: Troubleshooting and Optimization\n",
    "\n",
    "## Common Issues and Solutions\n",
    "\n",
    "1. **Installation Issues**\n",
    "   - Use a proper package manager like Conda or uv (as you did)\n",
    "   - Install PyTorch with: `pip install torch torchvision torchaudio`\n",
    "   - For Apple Silicon: Make sure you have Python for arm64, not x86_64\n",
    "\n",
    "2. **MPS-specific Issues**\n",
    "   - MPS is only available on macOS 12.3+\n",
    "   - Some operations aren't supported on MPS yet; fallback to CPU for these\n",
    "   - If you see \"MPS backend out of memory\" errors, reduce batch size\n",
    "\n",
    "3. **Performance Optimization**\n",
    "   - Use mixed precision when possible (`torch.cuda.amp` equivalent functionality)\n",
    "   - Adjust batch sizes based on available memory\n",
    "   - Avoid unnecessary data transfers between CPU and MPS device\n",
    "   - Use `torch.compile()` for PyTorch 2.0+ models\n",
    "\n",
    "4. **Memory Management**\n",
    "   - Monitor memory with Activity Monitor\n",
    "   - Call `torch.cuda.empty_cache()` equivalent for MPS\n",
    "   - Use smaller precision when possible (FP16 instead of FP32)\n",
    "\n",
    "## Installing the Right PyTorch Version\n",
    "\n",
    "```bash\n",
    "# For pip\n",
    "pip install torch torchvision torchaudio\n",
    "\n",
    "# For Conda\n",
    "conda install pytorch torchvision torchaudio -c pytorch\n",
    "```\n",
    "\n",
    "## PyTorch MPS References\n",
    "\n",
    "- [PyTorch MPS Documentation](https://pytorch.org/docs/stable/notes/mps.html)\n",
    "- [Apple Developer MPS Documentation](https://developer.apple.com/metal/pytorch/)\n",
    "- [PyTorch Forums - MPS Discussions](https://discuss.pytorch.org/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
