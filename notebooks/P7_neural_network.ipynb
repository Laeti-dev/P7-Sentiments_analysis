{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "709265a1",
   "metadata": {},
   "source": [
    "# Strategies\n",
    "\n",
    "This notebook aims to test a neural network using different word embedding methods.\n",
    "\n",
    "**Word embedding methods:**\n",
    "\n",
    "| Method       | Definition                                                                 | Advantages                                                                                     | Limitations                                                                 |\n",
    "|--------------|-----------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------|\n",
    "| Word2Vec     | Represents words as dense vectors in a continuous space, capturing semantic relationships based on context. | Effective for capturing semantic relationships; widely used in NLP tasks.                     | Requires large datasets; struggles with out-of-vocabulary words.            |\n",
    "| Glove        | Generates word embeddings by factorizing a co-occurrence matrix, capturing both local and global semantic relationships. | Captures both local and global context; effective for text classification and sentiment analysis. | Computationally expensive; requires pre-computed co-occurrence statistics.  |\n",
    "| USE (Universal Sentence Encoder) | Produces embeddings for entire sentences rather than individual words, leveraging deep learning models. | Captures sentence-level semantics; pre-trained models available for quick use.                | Higher computational cost; less effective for word-level tasks.             |\n",
    "| BERT |  |                 |              |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dded7f0",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c6b913f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/ikusawalaetitia/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ikusawalaetitia/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/ikusawalaetitia/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version: 2.16.2\n",
      "GPU disponible : [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import warnings\n",
    "from collections import Counter\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "# NLTK\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "\n",
    "# scikit-learn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,\n",
    "                             fbeta_score, make_scorer, matthews_corrcoef, balanced_accuracy_score,\n",
    "                             classification_report, confusion_matrix, roc_auc_score, roc_curve)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Add Hugging Face transformers for BERT\n",
    "# try:\n",
    "#     from transformers import BertTokenizer, TFBertModel\n",
    "# except ImportError:\n",
    "#     print(\"Installing transformers library...\")\n",
    "#     import sys\n",
    "#     !{sys.executable} -m pip install transformers\n",
    "#     from transformers import BertTokenizer, TFBertModel\n",
    "\n",
    "print(\"Version:\", tf.__version__)\n",
    "print(\"GPU disponible :\", tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "\n",
    "import mlflow\n",
    "import mlflow.keras\n",
    "\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.family'] = 'DejaVu Sans'\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79a01571",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.expand_frame_repr', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43516f9",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca74829",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51e4c1b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>ids</th>\n",
       "      <th>date</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "      <th>processed_text_lem</th>\n",
       "      <th>advanced_processed_text_lem</th>\n",
       "      <th>processed_text_stem</th>\n",
       "      <th>advanced_processed_text_stem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2065378704</td>\n",
       "      <td>2009-06-07 08:28:25</td>\n",
       "      <td>juliaindelicate</td>\n",
       "      <td>@christt I really wanted to go  i didn't get t...</td>\n",
       "      <td>&lt; mention &gt; really wanted go not get go anal b...</td>\n",
       "      <td>&lt; mention &gt; really wanted go not get go anal b...</td>\n",
       "      <td>&lt; mention &gt; realli want go not get go ani anal...</td>\n",
       "      <td>&lt; mention &gt; realli want go not get go ani anal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2198776518</td>\n",
       "      <td>2009-06-16 16:33:19</td>\n",
       "      <td>hxcfairy</td>\n",
       "      <td>Finally home from my sisters and I'm so tired</td>\n",
       "      <td>finally home sister tired</td>\n",
       "      <td>finally home sister tired</td>\n",
       "      <td>final home sister tire</td>\n",
       "      <td>final home sister tire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2258118867</td>\n",
       "      <td>2009-06-20 15:24:59</td>\n",
       "      <td>TranceGemini613</td>\n",
       "      <td>I really, really wish I'd known all the stuff ...</td>\n",
       "      <td>really really wish would known stuff james tel...</td>\n",
       "      <td>really really wish would known stuff james tel...</td>\n",
       "      <td>realli realli wish would known stuff jame tell...</td>\n",
       "      <td>realli realli wish would known stuff jame tell...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1755733793</td>\n",
       "      <td>2009-05-10 09:53:26</td>\n",
       "      <td>jwatkins5377</td>\n",
       "      <td>Can officially say I have been at work EVERYDA...</td>\n",
       "      <td>officially say work everyday week ... bad toda...</td>\n",
       "      <td>officially say work everyday week ... bad toda...</td>\n",
       "      <td>offici say work everyday thi week ... bad toda...</td>\n",
       "      <td>offici say work everyday thi week ... bad toda...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1881082418</td>\n",
       "      <td>2009-05-22 03:32:26</td>\n",
       "      <td>davelee</td>\n",
       "      <td>#FirstRecord The Mr Blobby single  Sorry folks.</td>\n",
       "      <td>&lt; hashtag &gt; mr blobby single sorry folk .</td>\n",
       "      <td>&lt; hashtag &gt; mr lobby single sorry folk .</td>\n",
       "      <td>&lt; hashtag &gt; mr blobbi singl sorri folk .</td>\n",
       "      <td>&lt; hashtag &gt; mr lobbi singl sorri folk .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>2045678642</td>\n",
       "      <td>2009-06-05 11:04:38</td>\n",
       "      <td>johnny101</td>\n",
       "      <td>My turkey and ham salad wrap is not sitting we...</td>\n",
       "      <td>turkey ham salad wrap not sitting well .</td>\n",
       "      <td>turkey ham salad wrap not sitting well .</td>\n",
       "      <td>turkey ham salad wrap not sit well .</td>\n",
       "      <td>turkey ham salad wrap not sit well .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>2323712552</td>\n",
       "      <td>2009-06-25 01:17:50</td>\n",
       "      <td>Matt_Whiting</td>\n",
       "      <td>Footy Trainin Was called off     Now i Dont kn...</td>\n",
       "      <td>footy trainin wa called dont know ? laugh loud</td>\n",
       "      <td>footy training wa called not know ? lot</td>\n",
       "      <td>footi trainin wa call dont know ? laugh loud</td>\n",
       "      <td>footi train wa call not know ? lot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>2200119346</td>\n",
       "      <td>2009-06-16 18:27:54</td>\n",
       "      <td>JoePaley17</td>\n",
       "      <td>Just got bacl from ftbl summer school tomorrow</td>\n",
       "      <td>got bacl ftbl summer school tomorrow</td>\n",
       "      <td>got back feel summer school tomorrow</td>\n",
       "      <td>got bacl ftbl summer school tomorrow</td>\n",
       "      <td>got back feel summer school tomorrow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>2247872282</td>\n",
       "      <td>2009-06-19 19:45:15</td>\n",
       "      <td>trexsandwich</td>\n",
       "      <td>@writesfortea Cool teachers are the best, but ...</td>\n",
       "      <td>&lt; mention &gt; cool teacher best never lasted sch...</td>\n",
       "      <td>&lt; mention &gt; cool teacher best never lasted sch...</td>\n",
       "      <td>&lt; mention &gt; cool teacher best never last schoo...</td>\n",
       "      <td>&lt; mention &gt; cool teacher best never last schoo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>1835988160</td>\n",
       "      <td>2009-05-18 07:09:13</td>\n",
       "      <td>renzzee</td>\n",
       "      <td>@rediska08 awts.  friday, parteeey! hahaha! sa...</td>\n",
       "      <td>&lt; mention &gt; awts . friday parteey ! hahaha ! s...</td>\n",
       "      <td>&lt; mention &gt; awts . friday parteey ! hahaha ! s...</td>\n",
       "      <td>&lt; mention &gt; awt . friday parteey ! hahaha ! sa...</td>\n",
       "      <td>&lt; mention &gt; awt . friday parteey ! hahaha ! sa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target         ids                 date             user  \\\n",
       "0       0  2065378704  2009-06-07 08:28:25  juliaindelicate   \n",
       "1       0  2198776518  2009-06-16 16:33:19         hxcfairy   \n",
       "2       0  2258118867  2009-06-20 15:24:59  TranceGemini613   \n",
       "3       0  1755733793  2009-05-10 09:53:26     jwatkins5377   \n",
       "4       0  1881082418  2009-05-22 03:32:26          davelee   \n",
       "5       0  2045678642  2009-06-05 11:04:38        johnny101   \n",
       "6       0  2323712552  2009-06-25 01:17:50     Matt_Whiting   \n",
       "7       0  2200119346  2009-06-16 18:27:54       JoePaley17   \n",
       "8       0  2247872282  2009-06-19 19:45:15     trexsandwich   \n",
       "9       0  1835988160  2009-05-18 07:09:13          renzzee   \n",
       "\n",
       "                                                text  \\\n",
       "0  @christt I really wanted to go  i didn't get t...   \n",
       "1     Finally home from my sisters and I'm so tired    \n",
       "2  I really, really wish I'd known all the stuff ...   \n",
       "3  Can officially say I have been at work EVERYDA...   \n",
       "4    #FirstRecord The Mr Blobby single  Sorry folks.   \n",
       "5  My turkey and ham salad wrap is not sitting we...   \n",
       "6  Footy Trainin Was called off     Now i Dont kn...   \n",
       "7    Just got bacl from ftbl summer school tomorrow    \n",
       "8  @writesfortea Cool teachers are the best, but ...   \n",
       "9  @rediska08 awts.  friday, parteeey! hahaha! sa...   \n",
       "\n",
       "                                  processed_text_lem  \\\n",
       "0  < mention > really wanted go not get go anal b...   \n",
       "1                          finally home sister tired   \n",
       "2  really really wish would known stuff james tel...   \n",
       "3  officially say work everyday week ... bad toda...   \n",
       "4          < hashtag > mr blobby single sorry folk .   \n",
       "5           turkey ham salad wrap not sitting well .   \n",
       "6     footy trainin wa called dont know ? laugh loud   \n",
       "7               got bacl ftbl summer school tomorrow   \n",
       "8  < mention > cool teacher best never lasted sch...   \n",
       "9  < mention > awts . friday parteey ! hahaha ! s...   \n",
       "\n",
       "                         advanced_processed_text_lem  \\\n",
       "0  < mention > really wanted go not get go anal b...   \n",
       "1                          finally home sister tired   \n",
       "2  really really wish would known stuff james tel...   \n",
       "3  officially say work everyday week ... bad toda...   \n",
       "4           < hashtag > mr lobby single sorry folk .   \n",
       "5           turkey ham salad wrap not sitting well .   \n",
       "6            footy training wa called not know ? lot   \n",
       "7               got back feel summer school tomorrow   \n",
       "8  < mention > cool teacher best never lasted sch...   \n",
       "9  < mention > awts . friday parteey ! hahaha ! s...   \n",
       "\n",
       "                                 processed_text_stem  \\\n",
       "0  < mention > realli want go not get go ani anal...   \n",
       "1                             final home sister tire   \n",
       "2  realli realli wish would known stuff jame tell...   \n",
       "3  offici say work everyday thi week ... bad toda...   \n",
       "4           < hashtag > mr blobbi singl sorri folk .   \n",
       "5               turkey ham salad wrap not sit well .   \n",
       "6       footi trainin wa call dont know ? laugh loud   \n",
       "7               got bacl ftbl summer school tomorrow   \n",
       "8  < mention > cool teacher best never last schoo...   \n",
       "9  < mention > awt . friday parteey ! hahaha ! sa...   \n",
       "\n",
       "                        advanced_processed_text_stem  \n",
       "0  < mention > realli want go not get go ani anal...  \n",
       "1                             final home sister tire  \n",
       "2  realli realli wish would known stuff jame tell...  \n",
       "3  offici say work everyday thi week ... bad toda...  \n",
       "4            < hashtag > mr lobbi singl sorri folk .  \n",
       "5               turkey ham salad wrap not sit well .  \n",
       "6                 footi train wa call not know ? lot  \n",
       "7               got back feel summer school tomorrow  \n",
       "8  < mention > cool teacher best never last schoo...  \n",
       "9  < mention > awt . friday parteey ! hahaha ! sa...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_to_sample = \"../data/processed_sample_tweets.csv\"\n",
    "\n",
    "sample_df = pd.read_csv(path_to_sample, encoding='utf-8')\n",
    "# Display the first few rows of the dataframe\n",
    "sample_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd90c7af",
   "metadata": {},
   "source": [
    "## Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85302e74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_lem shape: (30000,)\n",
      "X_test_lem shape: (10000,)\n",
      "y_train_lem shape: (30000,)\n",
      "y_test_lem shape: (10000,)\n",
      "X_train_stem shape: (30000,)\n",
      "X_val_stem shape: (10000,)\n",
      "X_test_stem shape: (10000,)\n",
      "y_train_stem shape: (30000,)\n",
      "X_val_stem shape: (10000,)\n",
      "y_test_stem shape: (10000,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define features (X) and target (y)\n",
    "X_lem = sample_df['processed_text_lem']\n",
    "X_stem = sample_df['processed_text_stem']\n",
    "\n",
    "y = sample_df['target']\n",
    "\n",
    "# Split the data into training, testing and validation sets\n",
    "X_temp_lem, X_test_lem, y_temp_lem, y_test_lem = train_test_split(\n",
    "    X_lem, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "X_train_lem, X_val_lem, y_train_lem, y_val_lem = train_test_split(\n",
    "    X_temp_lem, y_temp_lem,\n",
    "    test_size=0.25,  # 0.25 x 0.8 = 0.2\n",
    "    random_state=42,\n",
    "    stratify=y_temp_lem\n",
    ")\n",
    "\n",
    "# Display the shapes of the resulting datasets\n",
    "print(f\"X_train_lem shape: {X_train_lem.shape}\")\n",
    "print(f\"X_test_lem shape: {X_test_lem.shape}\")\n",
    "print(f\"y_train_lem shape: {y_train_lem.shape}\")\n",
    "print(f\"y_test_lem shape: {y_test_lem.shape}\")\n",
    "\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_temp_stem, X_test_stem, y_temp_stem, y_test_stem = train_test_split(\n",
    "    X_stem, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "X_train_stem, X_val_stem, y_train_stem, y_val_stem = train_test_split(\n",
    "    X_temp_stem, y_temp_stem,\n",
    "    test_size=0.25,  # 0.25 x 0.8 = 0.2\n",
    "    random_state=42,\n",
    "    stratify=y_temp_stem\n",
    ")\n",
    "\n",
    "\n",
    "# Display the shapes of the resulting datasets\n",
    "print(f\"X_train_stem shape: {X_train_stem.shape}\")\n",
    "print(f\"X_val_stem shape: {X_val_stem.shape}\")\n",
    "print(f\"X_test_stem shape: {X_test_stem.shape}\")\n",
    "print(f\"y_train_stem shape: {y_train_stem.shape}\")\n",
    "print(f\"X_val_stem shape: {X_val_stem.shape}\")\n",
    "print(f\"y_test_stem shape: {y_test_stem.shape}\")\n",
    "\n",
    "X_train_shape = X_train_lem.shape\n",
    "X_val_shape = X_val_lem.shape\n",
    "X_test_shape = X_test_lem.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573c479d",
   "metadata": {},
   "source": [
    "## MLFlow setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2822e3f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLflow Tracking URI: http://localhost:8080\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import time\n",
    "\n",
    "import mlflow\n",
    "from mlflow.models import infer_signature\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# For scikit-learn\n",
    "mlflow.sklearn.autolog()\n",
    "\n",
    "# Configuring MLflow\n",
    "load_dotenv()\n",
    "tracking_uri = os.getenv(\"MLFLOW_TRACKING_URI\")\n",
    "mlflow.set_tracking_uri(tracking_uri)\n",
    "print(f\"MLflow Tracking URI: {tracking_uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c459fb4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created new experiment 'P7-Sentiments_Analysis_neural_network' with ID: 915290206608231433\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='mlflow-artifacts:/915290206608231433', creation_time=1758900314087, experiment_id='915290206608231433', last_update_time=1758900314087, lifecycle_stage='active', name='P7-Sentiments_Analysis_neural_network', tags={}>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new MLflow Experiment\n",
    "experiment_name = \"P7-Sentiments_Analysis_neural_network\"\n",
    "\n",
    "# Check if the experiment exists, create it if it doesn't\n",
    "experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "if experiment is None:\n",
    "    experiment_id = mlflow.create_experiment(experiment_name)\n",
    "    print(f\"Created new experiment '{experiment_name}' with ID: {experiment_id}\")\n",
    "else:\n",
    "    experiment_id = experiment.experiment_id\n",
    "    print(f\"Using existing experiment '{experiment_name}' with ID: {experiment_id}\")\n",
    "\n",
    "# Set the experiment\n",
    "mlflow.set_experiment(experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b11b80",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cdce05e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetVectorizer:\n",
    "    def __init__(self, preprocessor: str = None, vectoriser: str = 'w2v'):\n",
    "        self.preprocessor = preprocessor\n",
    "        self.vectoriser = vectoriser\n",
    "        self.stemmer = PorterStemmer()\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "        # Initialize vectorizers\n",
    "        if vectoriser == 'w2v':\n",
    "            from gensim.models import Word2Vec\n",
    "            self.model = None # Will be initialized during fit\n",
    "        elif vectoriser == 'fasttext':\n",
    "            from gensim.models import FastText\n",
    "            self.model = FastText(vector_size=100, window=5, min_count=1, workers=4)\n",
    "        elif vectoriser == 'use':\n",
    "            import tensorflow_hub as hub\n",
    "            self.model = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
    "        elif vectoriser == 'bert':\n",
    "            # Initialize BERT tokenizer and model\n",
    "            from transformers import BertTokenizer, TFBertModel\n",
    "            self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "            self.model = TFBertModel.from_pretrained('bert-base-uncased')\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported vectoriser. Choose from 'w2v', 'fasttext', 'use', or 'bert'.\")\n",
    "\n",
    "    def preprocess(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Cleans and preprocesses a single text string by replacing URLs, mentions, and hashtags,\n",
    "        converting to lowercase, removing special characters, and removing stopwords.\n",
    "\n",
    "        Args:\n",
    "            text (str): The text string to process.\n",
    "\n",
    "        Returns:\n",
    "            str: The processed text string.\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        if pd.isna(text):\n",
    "            return \"\"\n",
    "        # Replace URLs with <URL>\n",
    "        processed = re.sub(r'https?://\\S+', '<URL>', text)\n",
    "        # Replace mentions with <MENTION>\n",
    "        processed = re.sub(r'@[A-Za-z0-9_]+', '<MENTION>', processed)\n",
    "        # Separate # from word and replace the word with <HASHTAG>\n",
    "        processed = re.sub(r'#([A-Za-z0-9_]+)', r'#<HASHTAG>', processed)\n",
    "\n",
    "        # Convert text to lowercase\n",
    "        processed = processed.lower()\n",
    "\n",
    "        # Remove special characters and numbers, keeping !, ?, and ellipsis (...)\n",
    "        # Also keeps the placeholders <URL>, <MENTION>, <HASHTAG>\n",
    "        processed = re.sub(r'[^a-z0-9\\s.!?<>#]', '', processed)\n",
    "\n",
    "        # Tokenize the text\n",
    "        tokens = word_tokenize(processed)\n",
    "\n",
    "        # Initialize lemmatizer\n",
    "        if self.preprocessor == 'lemmatization':\n",
    "            tokens = [self.lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "        # Initialize stemmer\n",
    "        if self.preprocessor == 'stemming':\n",
    "            tokens = [self.stemmer.stem(token) for token in tokens]\n",
    "\n",
    "\n",
    "        # Define negative words that should not be removed\n",
    "        negative_words = {\n",
    "            'no', 'not', 'nor', \"don't\", \"aren't\", \"couldn't\", \"didn't\", \"doesn't\",\n",
    "            \"hadn't\", \"hasn't\", \"haven't\", \"isn't\", \"mightn't\", \"mustn't\",\n",
    "            \"needn't\", \"shan't\", \"shouldn't\", \"wasn't\", \"weren't\", \"won't\", \"wouldn't\",\n",
    "            \"never\", \"none\", \"nobody\", \"nothing\", \"nowhere\", \"neither\"\n",
    "        }\n",
    "        # Create a set of stopwords to remove, excluding the negative words\n",
    "        stop_words_to_remove = set(stopwords.words('english')) - negative_words\n",
    "\n",
    "        # remove stopwords, and join back to string\n",
    "        filtered_tokens = [word for word in tokens if word not in stop_words_to_remove]\n",
    "\n",
    "        return ' '.join(filtered_tokens)\n",
    "\n",
    "    def fit_transform(self, texts: pd.Series):\n",
    "        \"\"\"\n",
    "        Fits the vectorizer model (if applicable) and transforms the input texts into vectors.\n",
    "\n",
    "        Args:\n",
    "            texts (pd.Series): Series of text strings to fit and transform.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Array of vectorized texts.\n",
    "        \"\"\"\n",
    "        processed_texts = texts.apply(self.preprocess).tolist()\n",
    "\n",
    "        if self.vectoriser == 'w2v':\n",
    "            from gensim.models import Word2Vec\n",
    "            tokenized_texts = [text.split() for text in processed_texts]\n",
    "            self.model = Word2Vec(sentences=tokenized_texts, vector_size=100, window=5, min_count=1, workers=4)\n",
    "            vectors = np.array([np.mean([self.model.wv[word] for word in text.split() if word in self.model.wv] or [np.zeros(100)], axis=0) for text in processed_texts])\n",
    "        elif self.vectoriser == 'fasttext':\n",
    "            tokenized_texts = [text.split() for text in processed_texts]\n",
    "            self.model.build_vocab(tokenized_texts, update=True)\n",
    "            self.model.train(tokenized_texts, total_examples=len(tokenized_texts), epochs=self.model.epochs)\n",
    "            vectors = np.array([np.mean([self.model.wv[word] for word in text.split() if word in self.model.wv] or [np.zeros(100)], axis=0) for text in processed_texts])\n",
    "        elif self.vectoriser == 'use':\n",
    "            vectors = self.model(processed_texts).numpy()\n",
    "        elif self.vectoriser == 'bert':\n",
    "            # For BERT, we need to tokenize the text and return the encoded features\n",
    "            # This returns a batch of token IDs, attention masks, and token type IDs\n",
    "            encoded_inputs = self.tokenizer(\n",
    "                processed_texts,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                max_length=self.max_length,\n",
    "                return_tensors='tf'\n",
    "            )\n",
    "\n",
    "            # Get BERT embeddings - using CLS token (first token) as the sentence representation\n",
    "            outputs = self.model(encoded_inputs)\n",
    "            # We'll use the CLS token (first token) embeddings as the sentence representation\n",
    "            vectors = outputs.last_hidden_state[:, 0, :].numpy()  # Shape: (batch_size, hidden_size=768)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported vectoriser. Choose from 'w2v', 'fasttext', 'use', or 'bert'.\")\n",
    "\n",
    "        # Chech for 2D array\n",
    "        if vectors.ndim == 1:\n",
    "            vectors = vectors.reshape(-1, 1)\n",
    "        elif vectors.ndim == 2 and vectors.shape[1] == 1:\n",
    "            vectors = vectors.reshape(-1, vectors.shape[1])\n",
    "\n",
    "        return vectors\n",
    "\n",
    "    def transform(self, texts: pd.Series):\n",
    "        \"\"\"\n",
    "        Transforms the input texts into vectors using the fitted vectorizer model.\n",
    "\n",
    "        Args:\n",
    "            texts (pd.Series): Series of text strings to transform.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Array of vectorized texts.\n",
    "        \"\"\"\n",
    "        processed_texts = texts.apply(self.preprocess).tolist()\n",
    "\n",
    "        if self.vectoriser == 'w2v':\n",
    "            if self.model is None:\n",
    "                raise ValueError(\"The Word2Vec model has not been fitted. Call fit_transform first.\")\n",
    "            vectors = np.array([np.mean([self.model.wv[word] for word in text.split() if word in self.model.wv] or [np.zeros(100)], axis=0) for text in processed_texts])\n",
    "        elif self.vectoriser == 'fasttext':\n",
    "            if self.model is None:\n",
    "                raise ValueError(\"The FastText model has not been fitted. Call fit_transform first.\")\n",
    "            vectors = np.array([np.mean([self.model.wv[word] for word in text.split() if word in self.model.wv] or [np.zeros(100)], axis=0) for text in processed_texts])\n",
    "        elif self.vectoriser == 'use':\n",
    "            vectors = self.model(processed_texts).numpy()\n",
    "        elif self.vectoriser == 'bert':\n",
    "            # For BERT, we need to tokenize the text and return the encoded features\n",
    "            encoded_inputs = self.tokenizer(\n",
    "                processed_texts,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                max_length=self.max_length,\n",
    "                return_tensors='tf'\n",
    "            )\n",
    "\n",
    "            # Get BERT embeddings - using CLS token (first token) as the sentence representation\n",
    "            outputs = self.model(encoded_inputs)\n",
    "            vectors = outputs.last_hidden_state[:, 0, :].numpy()  # Shape: (batch_size, hidden_size=768)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported vectoriser. Choose from 'w2v', 'fasttext', 'use', or 'bert'.\")\n",
    "\n",
    "        return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bfe0a91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Embedding, Bidirectional  # Ajout de Bidirectional\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from gensim.models import Word2Vec, FastText\n",
    "\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "class LSTMTweetClassifier:\n",
    "    def __init__(self, embedding:str, embedding_dim: int, lstm_units: int=128, max_length: int = 100):\n",
    "        self.embedding = embedding\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.lstm_units = lstm_units\n",
    "        self.max_length = max_length\n",
    "        self.tokenizer = Tokenizer()\n",
    "        self.model = None\n",
    "        self.word_index = self.tokenizer.word_index\n",
    "\n",
    "    def build_embedding_matrix(self):\n",
    "        if self.embedding == 'w2v':\n",
    "            from gensim.models import Word2Vec\n",
    "            embedding_model = Word2Vec.load(\"word2vec.model\")\n",
    "        elif self.embedding == 'fasttext':\n",
    "            from gensim.models import FastText\n",
    "            embedding_model = FastText.load(\"fasttext.model\")\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported embedding type. Choose 'w2v' or 'fasttext'.\")\n",
    "\n",
    "        vocab_size = len(self.word_index) + 1  # +1 for padding token\n",
    "        embedding_matrix = np.zeros((vocab_size, self.embedding_dim))\n",
    "\n",
    "        for word, i in self.word_index.items():\n",
    "            if word in embedding_model.wv:\n",
    "                embedding_matrix[i] = embedding_model.wv[word]\n",
    "            else:\n",
    "                embedding_matrix[i] = np.random.normal(size=(self.embedding_dim,))\n",
    "\n",
    "        return embedding_matrix\n",
    "\n",
    "    def build_model(self, vocab_size: int, embedding_matrix: np.ndarray):\n",
    "        \"\"\"Builds the LSTM model architecture.\n",
    "\n",
    "        Args:\n",
    "            vocab_size (int): Size of the vocabulary.\n",
    "            embedding_matrix (np.ndarray): Pre-trained embedding matrix.\n",
    "        Returns:\n",
    "            keras.Model: Compiled LSTM model.\n",
    "        \"\"\"\n",
    "        # Initialize the model\n",
    "        model = Sequential()\n",
    "        # Add embedding layer with pre-trained weights\n",
    "        model.add(Embedding(input_dim=vocab_size,\n",
    "                            output_dim=self.embedding_dim,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=self.max_length,\n",
    "                            trainable=False))  # Freezing the embedding layer\n",
    "        # Add a Bidirectional LSTM layer\n",
    "        model.add(Bidirectional(LSTM(self.lstm_units, return_sequences=False)))\n",
    "        # Add a Dense output layer with sigmoid activation for binary classification\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "        model.compile(loss='binary_crossentropy',\n",
    "                      optimizer=Adam(learning_rate=0.001),\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "        self.model = model\n",
    "        return model\n",
    "\n",
    "    def get_metrics(self, y_true, y_pred):\n",
    "        \"\"\"Calculates and returns various classification metrics.\n",
    "\n",
    "        Args:\n",
    "            y_true (np.ndarray): True labels.\n",
    "            y_pred (np.ndarray): Predicted labels.\n",
    "        Returns:\n",
    "            dict: Dictionary containing accuracy, precision, recall, f1-score, and MCC.\n",
    "        \"\"\"\n",
    "        accuracy = np.mean((y_pred > 0.5).flatten == y_true)\n",
    "        specificity = np.sum((y_pred < 0.5).flatten & (y_true == 0)) / np.sum(y_true == 0)\n",
    "        auc = roc_auc_score(y_true, y_pred)\n",
    "        mlflow.log_metric(\"accuracy\", accuracy)\n",
    "        mlflow.log_metric(\"specificity\", specificity)\n",
    "        mlflow.log_metric(\"auc\", auc)\n",
    "        return {\n",
    "            'accuracy': accuracy,\n",
    "            'specificity': specificity,\n",
    "            'auc': auc\n",
    "        }\n",
    "\n",
    "    def train_and_evaluate(self, X_train, y_train, X_val, y_val, epochs=10, batch_size=32):\n",
    "        \"\"\"Trains the LSTM model and evaluates it on the validation set.\n",
    "\n",
    "        Args:\n",
    "            X_train (np.ndarray): Training feature vectors.\n",
    "            y_train (np.ndarray): Training labels.\n",
    "            X_val (np.ndarray): Validation feature vectors.\n",
    "            y_val (np.ndarray): Validation labels.\n",
    "            epochs (int, optional): Number of training epochs. Defaults to 10.\n",
    "            batch_size (int, optional): Batch size for training. Defaults to 32.\n",
    "        Returns:\n",
    "            dict: Dictionary containing evaluation metrics on the validation set.\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"The model has not been built. Call build_model first.\")\n",
    "        with mlflow.start_run(run_name=f\"LSTM_{self.embedding}_{self.epochs}epochs_{self.lstm_units}units\", nested= True):\n",
    "            # log model parameters\n",
    "            mlflow.log_param(\"embedding\", self.embedding)\n",
    "            mlflow.log_param(\"embedding_dim\", self.embedding_dim)\n",
    "            mlflow.log_param(\"lstm_units\", self.lstm_units)\n",
    "            mlflow.log_param(\"max_length\", self.max_length)\n",
    "            mlflow.log_param(\"epochs\", epochs)\n",
    "            mlflow.log_param(\"batch_size\", batch_size)\n",
    "\n",
    "            start_time = time.time()\n",
    "            history = self.model.fit(X_train, y_train,\n",
    "                                     validation_data=(X_val, y_val),\n",
    "                                     epochs=epochs,\n",
    "                                     batch_size=batch_size,\n",
    "                                     verbose=1)\n",
    "            training_time = time.time() - start_time\n",
    "            mlflow.log_metric(\"training_time\", training_time)\n",
    "\n",
    "            # Log training and validation metrics for each epoch\n",
    "            for epoch in range(epochs):\n",
    "                mlflow.log_metric(\"train_loss\", history.history['loss'][epoch], step=epoch)\n",
    "                mlflow.log_metric(\"train_accuracy\", history.history['accuracy'][epoch], step=epoch)\n",
    "                mlflow.log_metric(\"val_loss\", history.history['val_loss'][epoch], step=epoch)\n",
    "                mlflow.log_metric(\"val_accuracy\", history.history['val_accuracy'][epoch], step=epoch)\n",
    "\n",
    "            # Evaluate on validation set\n",
    "            start_prediction_time = time.time()\n",
    "            y_val_pred = self.model.predict(X_val).flatten()\n",
    "            prediction_time = time.time() - start_prediction_time\n",
    "            mlflow.log_metric(\"prediction_time\", prediction_time)\n",
    "\n",
    "            # Calulate validation metrics\n",
    "            val_metrics = {\n",
    "                'accuracy': accuracy_score(y_val, (y_val_pred > 0.5).astype(int)),\n",
    "                'precision': precision_score(y_val, (y_val_pred > 0.5).astype(int)),\n",
    "                'recall': recall_score(y_val, (y_val_pred > 0.5).astype(int)),\n",
    "                'f1_score': f1_score(y_val, (y_val_pred > 0.5).astype(int)),\n",
    "                'specificity': np.sum((y_val_pred < 0.5).flatten() & (y_val == 0)) / np.sum(y_val == 0),\n",
    "                'roc_auc': roc_auc_score(y_val, y_val_pred),\n",
    "                'prediction_time': prediction_time\n",
    "            }\n",
    "\n",
    "            mlflow.log_table(pd.DataFrame([val_metrics]), \"validation_metrics.csv\")\n",
    "            for metric_name, metric_value in val_metrics.items():\n",
    "                mlflow.log_metric(metric_name, metric_value)\n",
    "\n",
    "            # ROC AUC proba\n",
    "            y_val_proba = self.model.predict(X_val).flatten()\n",
    "            fpr, tpr, thresholds = roc_curve(y_val, y_val_proba)\n",
    "            plt.figure()\n",
    "            plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % val_metrics['roc_auc'])\n",
    "            plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "            plt.xlim([0.0, 1.0])\n",
    "            plt.ylim([0.0, 1.05])\n",
    "            plt.xlabel('False Positive Rate')\n",
    "            plt.ylabel('True Positive Rate')\n",
    "            plt.title('Receiver Operating Characteristic')\n",
    "            plt.legend(loc=\"lower right\")\n",
    "            plt.savefig(\"roc_curve.png\")\n",
    "            mlflow.log_artifact(\"roc_curve.png\")\n",
    "            plt.close()\n",
    "\n",
    "            # Log the model\n",
    "            mlflow.keras.log_model(self.model, f\"LSTM_{self.embedding}_{self.lstm_units}units_model\")\n",
    "            run_id = mlflow.active_run().info.run_id\n",
    "            result= mlflow.register_model(f\"runs:/{run_id}/LSTM_{self.embedding}_{self.lstm_units}units_model\", f\"LSTM_{self.embedding}_{self.lstm_units}units_model\")\n",
    "\n",
    "    def fit(self, text: pd.Series, labels: pd.Series, test_size:float = 0.2, val_split:float = 0.2,epochs: int=10, batch_size: int=32):\n",
    "        # Tokenize and pad sequences\n",
    "        self.tokenizer.fit_on_texts(text)\n",
    "        sequences = self.tokenizer.texts_to_sequences(text)\n",
    "        padded_sequences = pad_sequences(sequences, maxlen=self.max_length, padding='post', truncating='post')\n",
    "\n",
    "        vocab_size = len(self.tokenizer.word_index) + 1  # +1 for padding token\n",
    "\n",
    "        # Build embedding matrix\n",
    "        embedding_matrix = self.build_embedding_matrix()\n",
    "\n",
    "        # Build the model\n",
    "        self.build_model(vocab_size, embedding_matrix)\n",
    "\n",
    "        # split the data into training/val and test sets\n",
    "        X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "            padded_sequences, labels, test_size=test_size, random_state=42, stratify=labels\n",
    "        )\n",
    "\n",
    "        # Split into train set into train and validation sets\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=val_split, random_state=42, stratify=y_temp)\n",
    "\n",
    "        print(f\"X_train shape: {X_train.shape}\")\n",
    "        print(f\"X_val shape: {X_val.shape}\")\n",
    "        print(f\"X_test shape: {X_test.shape}\")\n",
    "\n",
    "        # Train and evaluate the model\n",
    "        self.epochs = epochs\n",
    "        self.train_and_evaluate(padded_sequences, labels, X_val, y_val, epochs, batch_size)\n",
    "\n",
    "        # Final evaluation on the test set\n",
    "        y_test_pred = self.model.predict(X_test).flatten()\n",
    "        test_metrics = self.get_metrics(y_test, y_test_pred)\n",
    "        print(\"Test Metrics:\", test_metrics)\n",
    "        for metric_name, metric_value in test_metrics.items():\n",
    "            mlflow.log_metric(f\"test_{metric_name}\", metric_value)\n",
    "\n",
    "    def predict(self, texts: pd.Series):\n",
    "        \"\"\"Predicts the sentiment of the input texts.\n",
    "\n",
    "        Args:\n",
    "            texts (pd.Series): Series of text strings to predict.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Array of predicted labels.\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"The model has not been built or trained. Call fit first.\")\n",
    "\n",
    "        # Preprocess and tokenize the input texts\n",
    "        sequences = self.tokenizer.texts_to_sequences(texts)\n",
    "        padded_sequences = pad_sequences(sequences, maxlen=self.max_length, padding='post', truncating='post')\n",
    "\n",
    "        # Predict using the trained model\n",
    "        predictions = self.model.predict(padded_sequences).flatten()\n",
    "        predicted_labels = (predictions > 0.5).astype(int)\n",
    "\n",
    "        return predicted_labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c64b951",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4353b08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "config =[\n",
    "    {\n",
    "        'embedding': 'w2v',\n",
    "        'embedding_dim': 100,\n",
    "        'lstm_units': 128,\n",
    "        'max_length': 100,\n",
    "        'epochs': 5,\n",
    "        'batch_size': 32\n",
    "    },\n",
    "    {\n",
    "        'embedding': 'fasttext',\n",
    "        'embedding_dim': 100,\n",
    "        'lstm_units': 128,\n",
    "        'max_length': 100,\n",
    "        'epochs': 5,\n",
    "        'batch_size': 32\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924723e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_save_gensim_models(tokenized_corpus: pd.Series):\n",
    "    # --- Train and Save Word2Vec Model ---\n",
    "    print(\"Training Word2Vec model...\")\n",
    "    w2v_model = Word2Vec(sentences=tokenized_corpus,\n",
    "                        vector_size=100,  # Should match 'embedding_dim' in your config\n",
    "                        window=5,\n",
    "                        min_count=1,\n",
    "                        workers=4)\n",
    "    w2v_model.save(\"word2vec.model\")\n",
    "    print(\"Word2Vec model saved as word2vec.model\")\n",
    "\n",
    "\n",
    "    # --- Train and Save FastText Model ---\n",
    "    print(\"\\nTraining FastText model...\")\n",
    "    ft_model = FastText(sentences=tokenized_corpus,\n",
    "                        vector_size=100,  # Should match 'embedding_dim' in your config\n",
    "                        window=5,\n",
    "                        min_count=1,\n",
    "                        workers=4)\n",
    "    ft_model.save(\"fasttext.model\")\n",
    "    print(\"FastText model saved as fasttext.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e33ce85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LSTM with w2v embeddings...\n",
      "ðŸƒ View run LSTM_w2v_5epochs_128units_model at: http://localhost:8080/#/experiments/915290206608231433/runs/e614b4dcd5f04fb78bb3c95e0071fd8f\n",
      "ðŸ§ª View experiment at: http://localhost:8080/#/experiments/915290206608231433\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'word2vec.model'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     11\u001b[39m model = LSTMTweetClassifier(embedding=cfg[\u001b[33m'\u001b[39m\u001b[33membedding\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m     12\u001b[39m                             embedding_dim=cfg[\u001b[33m'\u001b[39m\u001b[33membedding_dim\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m     13\u001b[39m                             lstm_units=cfg[\u001b[33m'\u001b[39m\u001b[33mlstm_units\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m     14\u001b[39m                             max_length=cfg[\u001b[33m'\u001b[39m\u001b[33mmax_length\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Training\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocument\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mepochs\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mbatch_size\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m mlflow.end_run()\n\u001b[32m     20\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCompleted training for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcfg[\u001b[33m'\u001b[39m\u001b[33membedding\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m embeddings.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 182\u001b[39m, in \u001b[36mLSTMTweetClassifier.fit\u001b[39m\u001b[34m(self, text, labels, test_size, val_split, epochs, batch_size)\u001b[39m\n\u001b[32m    179\u001b[39m vocab_size = \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.tokenizer.word_index) + \u001b[32m1\u001b[39m  \u001b[38;5;66;03m# +1 for padding token\u001b[39;00m\n\u001b[32m    181\u001b[39m \u001b[38;5;66;03m# Build embedding matrix\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m182\u001b[39m embedding_matrix = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbuild_embedding_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    184\u001b[39m \u001b[38;5;66;03m# Build the model\u001b[39;00m\n\u001b[32m    185\u001b[39m \u001b[38;5;28mself\u001b[39m.build_model(vocab_size, embedding_matrix)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 23\u001b[39m, in \u001b[36mLSTMTweetClassifier.build_embedding_matrix\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.embedding == \u001b[33m'\u001b[39m\u001b[33mw2v\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m     22\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgensim\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Word2Vec\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m     embedding_model = \u001b[43mWord2Vec\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mword2vec.model\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.embedding == \u001b[33m'\u001b[39m\u001b[33mfasttext\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m     25\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgensim\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FastText\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/oc-projects/python/aiengineer/P7-Sentiments_analysis/.venv/lib/python3.12/site-packages/gensim/models/word2vec.py:1953\u001b[39m, in \u001b[36mWord2Vec.load\u001b[39m\u001b[34m(cls, rethrow, *args, **kwargs)\u001b[39m\n\u001b[32m   1934\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Load a previously saved :class:`~gensim.models.word2vec.Word2Vec` model.\u001b[39;00m\n\u001b[32m   1935\u001b[39m \n\u001b[32m   1936\u001b[39m \u001b[33;03mSee Also\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1950\u001b[39m \n\u001b[32m   1951\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1952\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1953\u001b[39m     model = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mWord2Vec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1954\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, Word2Vec):\n\u001b[32m   1955\u001b[39m         rethrow = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/oc-projects/python/aiengineer/P7-Sentiments_analysis/.venv/lib/python3.12/site-packages/gensim/utils.py:485\u001b[39m, in \u001b[36mSaveLoad.load\u001b[39m\u001b[34m(cls, fname, mmap)\u001b[39m\n\u001b[32m    481\u001b[39m logger.info(\u001b[33m\"\u001b[39m\u001b[33mloading \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m object from \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m, \u001b[38;5;28mcls\u001b[39m.\u001b[34m__name__\u001b[39m, fname)\n\u001b[32m    483\u001b[39m compress, subname = SaveLoad._adapt_by_suffix(fname)\n\u001b[32m--> \u001b[39m\u001b[32m485\u001b[39m obj = \u001b[43munpickle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    486\u001b[39m obj._load_specials(fname, mmap, compress, subname)\n\u001b[32m    487\u001b[39m obj.add_lifecycle_event(\u001b[33m\"\u001b[39m\u001b[33mloaded\u001b[39m\u001b[33m\"\u001b[39m, fname=fname)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/oc-projects/python/aiengineer/P7-Sentiments_analysis/.venv/lib/python3.12/site-packages/gensim/utils.py:1459\u001b[39m, in \u001b[36munpickle\u001b[39m\u001b[34m(fname)\u001b[39m\n\u001b[32m   1445\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34munpickle\u001b[39m(fname):\n\u001b[32m   1446\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Load object from `fname`, using smart_open so that `fname` can be on S3, HDFS, compressed etc.\u001b[39;00m\n\u001b[32m   1447\u001b[39m \n\u001b[32m   1448\u001b[39m \u001b[33;03m    Parameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1457\u001b[39m \n\u001b[32m   1458\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1459\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m   1460\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m _pickle.load(f, encoding=\u001b[33m'\u001b[39m\u001b[33mlatin1\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/oc-projects/python/aiengineer/P7-Sentiments_analysis/.venv/lib/python3.12/site-packages/smart_open/smart_open_lib.py:170\u001b[39m, in \u001b[36mopen\u001b[39m\u001b[34m(uri, mode, buffering, encoding, errors, newline, closefd, opener, compression, transport_params)\u001b[39m\n\u001b[32m    167\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m transport_params \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    168\u001b[39m     transport_params = {}\n\u001b[32m--> \u001b[39m\u001b[32m170\u001b[39m fobj = \u001b[43m_shortcut_open\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m    \u001b[49m\u001b[43muri\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbuffering\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbuffering\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    179\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    180\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m fobj\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/oc-projects/python/aiengineer/P7-Sentiments_analysis/.venv/lib/python3.12/site-packages/smart_open/smart_open_lib.py:368\u001b[39m, in \u001b[36m_shortcut_open\u001b[39m\u001b[34m(uri, mode, compression, buffering, encoding, errors, newline)\u001b[39m\n\u001b[32m    365\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mb\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m    366\u001b[39m     open_kwargs[\u001b[33m'\u001b[39m\u001b[33merrors\u001b[39m\u001b[33m'\u001b[39m] = errors\n\u001b[32m--> \u001b[39m\u001b[32m368\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_builtin_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocal_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffering\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbuffering\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mopen_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'word2vec.model'"
     ]
    }
   ],
   "source": [
    "experiment_name = \"P7-Sentiments_Analysis_LSTM\"\n",
    "document = sample_df[\"text\"]\n",
    "labels = sample_df[\"target\"]\n",
    "validation_metrics = []\n",
    "\n",
    "for cfg in config:\n",
    "    with mlflow.start_run(run_name=f\"LSTM_{cfg['embedding']}_{cfg['epochs']}epochs_{cfg['lstm_units']}units_model\", nested=False):\n",
    "\n",
    "        # Initialize the vectorizer\n",
    "        vectorizer = TweetVectorizer(preprocessor=cfg['embedding'], vectoriser=cfg['embedding'])\n",
    "\n",
    "        print(f\"Training LSTM with {cfg['embedding']} embeddings...\")\n",
    "        model = LSTMTweetClassifier(embedding=cfg['embedding'],\n",
    "                                    embedding_dim=cfg['embedding_dim'],\n",
    "                                    lstm_units=cfg['lstm_units'],\n",
    "                                    max_length=cfg['max_length'])\n",
    "\n",
    "        # Training\n",
    "        model.fit(document, labels, epochs=cfg['epochs'], batch_size=cfg['batch_size'])\n",
    "\n",
    "        mlflow.end_run()\n",
    "        print(f\"Completed training for {cfg['embedding']} embeddings.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "P7-kernel",
   "language": "python",
   "name": "p7-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
