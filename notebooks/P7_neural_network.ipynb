{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "709265a1",
   "metadata": {},
   "source": [
    "# Strategies\n",
    "\n",
    "This notebook aims to test a neural network using different word embedding methods.\n",
    "\n",
    "**Word embedding methods:**\n",
    "\n",
    "| Method       | Definition                                                                 | Advantages                                                                                     | Limitations                                                                 |\n",
    "|--------------|-----------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------|\n",
    "| Word2Vec     | Represents words as dense vectors in a continuous space, capturing semantic relationships based on context. | Effective for capturing semantic relationships; widely used in NLP tasks.                     | Requires large datasets; struggles with out-of-vocabulary words.            |\n",
    "| Glove        | Generates word embeddings by factorizing a co-occurrence matrix, capturing both local and global semantic relationships. | Captures both local and global context; effective for text classification and sentiment analysis. | Computationally expensive; requires pre-computed co-occurrence statistics.  |\n",
    "| USE (Universal Sentence Encoder) | Produces embeddings for entire sentences rather than individual words, leveraging deep learning models. | Captures sentence-level semantics; pre-trained models available for quick use.                | Higher computational cost; less effective for word-level tasks.             |\n",
    "| BERT |  |                 |              |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dded7f0",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6b913f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version: 2.16.2\n",
      "GPU disponible : [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/laetitiataddei/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/laetitiataddei/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/laetitiataddei/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import warnings\n",
    "from collections import Counter\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "# NLTK\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "\n",
    "# scikit-learn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,\n",
    "                             fbeta_score, make_scorer, matthews_corrcoef, balanced_accuracy_score,\n",
    "                             classification_report, confusion_matrix, roc_auc_score, roc_curve)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Add Hugging Face transformers for BERT\n",
    "try:\n",
    "    from transformers import BertTokenizer, TFBertModel\n",
    "except ImportError:\n",
    "    print(\"Installing transformers library...\")\n",
    "    import sys\n",
    "    !{sys.executable} -m pip install transformers\n",
    "    from transformers import BertTokenizer, TFBertModel\n",
    "\n",
    "print(\"Version:\", tf.__version__)\n",
    "print(\"GPU disponible :\", tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "\n",
    "import mlflow\n",
    "import mlflow.keras\n",
    "\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.family'] = 'DejaVu Sans'\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79a01571",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.expand_frame_repr', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43516f9",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca74829",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e4c1b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>ids</th>\n",
       "      <th>date</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>2009-04-06 22:19:45</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>2009-04-06 22:19:49</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>2009-04-06 22:19:53</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>2009-04-06 22:19:57</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>2009-04-06 22:19:57</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target         ids                date             user  \\\n",
       "0       0  1467810369 2009-04-06 22:19:45  _TheSpecialOne_   \n",
       "1       0  1467810672 2009-04-06 22:19:49    scotthamilton   \n",
       "2       0  1467810917 2009-04-06 22:19:53         mattycus   \n",
       "3       0  1467811184 2009-04-06 22:19:57          ElleCTF   \n",
       "4       0  1467811193 2009-04-06 22:19:57           Karoli   \n",
       "\n",
       "                                                text  \n",
       "0  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1  is upset that he can't update his Facebook by ...  \n",
       "2  @Kenichan I dived many times for the ball. Man...  \n",
       "3    my whole body feels itchy and like its on fire   \n",
       "4  @nationwideclass no, it's not behaving at all....  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the DataFrame: (1600000, 5)\n",
      "Number of unique target values: target\n",
      "0    800000\n",
      "4    800000\n",
      "Name: count, dtype: int64\n",
      "Number of unique target values after replacement: target\n",
      "0    800000\n",
      "1    800000\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "path_to_sample = \"../data/processed_sample_tweets.csv\"\n",
    "\n",
    "sample_df = pd.read_csv(path_to_sample, encoding='utf-8')\n",
    "# Display the first few rows of the dataframe\n",
    "sample_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd90c7af",
   "metadata": {},
   "source": [
    "## Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "85302e74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_lem shape: (30000,)\n",
      "X_test_lem shape: (10000,)\n",
      "y_train_lem shape: (30000,)\n",
      "y_test_lem shape: (10000,)\n",
      "X_train_stem shape: (30000,)\n",
      "X_val_stem shape: (10000,)\n",
      "X_test_stem shape: (10000,)\n",
      "y_train_stem shape: (30000,)\n",
      "X_val_stem shape: (10000,)\n",
      "y_test_stem shape: (10000,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define features (X) and target (y)\n",
    "X_lem = sample_df['processed_text_lem']\n",
    "X_stem = sample_df['processed_text_stem']\n",
    "\n",
    "y = sample_df['target']\n",
    "\n",
    "# Split the data into training, testing and validation sets\n",
    "X_temp_lem, X_test_lem, y_temp_lem, y_test_lem = train_test_split(\n",
    "    X_lem, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "X_train_lem, X_val_lem, y_train_lem, y_val_lem = train_test_split(\n",
    "    X_temp_lem, y_temp_lem,\n",
    "    test_size=0.25,  # 0.25 x 0.8 = 0.2\n",
    "    random_state=42,\n",
    "    stratify=y_temp_lem\n",
    ")\n",
    "\n",
    "# Display the shapes of the resulting datasets\n",
    "print(f\"X_train_lem shape: {X_train_lem.shape}\")\n",
    "print(f\"X_test_lem shape: {X_test_lem.shape}\")\n",
    "print(f\"y_train_lem shape: {y_train_lem.shape}\")\n",
    "print(f\"y_test_lem shape: {y_test_lem.shape}\")\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_temp_stem, X_test_stem, y_temp_stem, y_test_stem = train_test_split(\n",
    "    X_stem, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "X_train_stem, X_val_stem, y_train_stem, y_val_stem = train_test_split(\n",
    "    X_temp_stem, y_temp_stem,\n",
    "    test_size=0.25,  # 0.25 x 0.8 = 0.2\n",
    "    random_state=42,\n",
    "    stratify=y_temp_stem\n",
    ")\n",
    "\n",
    "\n",
    "# Display the shapes of the resulting datasets\n",
    "print(f\"X_train_stem shape: {X_train_stem.shape}\")\n",
    "print(f\"X_val_stem shape: {X_val_stem.shape}\")\n",
    "print(f\"X_test_stem shape: {X_test_stem.shape}\")\n",
    "print(f\"y_train_stem shape: {y_train_stem.shape}\")\n",
    "print(f\"X_val_stem shape: {X_val_stem.shape}\")\n",
    "print(f\"y_test_stem shape: {y_test_stem.shape}\")\n",
    "\n",
    "X_train_shape = X_train_lem.shape\n",
    "X_val_shape = X_val_lem.shape\n",
    "X_test_shape = X_test_lem.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573c479d",
   "metadata": {},
   "source": [
    "## MLFlow setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2822e3f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLflow Tracking URI: http://localhost:8080\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import time\n",
    "\n",
    "import mlflow\n",
    "from mlflow.models import infer_signature\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# For scikit-learn\n",
    "mlflow.sklearn.autolog()\n",
    "\n",
    "# Configuring MLflow\n",
    "load_dotenv()\n",
    "tracking_uri = os.getenv(\"MLFLOW_TRACKING_URI\")\n",
    "mlflow.set_tracking_uri(tracking_uri)\n",
    "print(f\"MLflow Tracking URI: {tracking_uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c459fb4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='mlflow-artifacts:/474291672324820532', creation_time=1757079621462, experiment_id='474291672324820532', last_update_time=1757079621462, lifecycle_stage='active', name='P7-Sentiments_Analysis', tags={}>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new MLflow Experiment\n",
    "mlflow.set_experiment(\"P7-Sentiments_Analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b11b80",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdce05e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetVectorizer:\n",
    "    def __init__(self, preprocessor: str = None, vectoriser: str = 'w2v'):\n",
    "        self.preprocessor = preprocessor\n",
    "        self.vectoriser = vectoriser\n",
    "        self.stemmer = PorterStemmer()\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "        # Initialize vectorizers\n",
    "        if vectoriser == 'w2v':\n",
    "            from gensim.models import Word2Vec\n",
    "            self.model = None # Will be initialized during fit\n",
    "        elif vectoriser == 'fasttext':\n",
    "            from gensim.models import FastText\n",
    "            self.model = FastText(vector_size=100, window=5, min_count=1, workers=4)\n",
    "        elif vectoriser == 'use':\n",
    "            import tensorflow_hub as hub\n",
    "            self.model = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
    "        elif vectoriser == 'bert':\n",
    "            # Initialize BERT tokenizer and model\n",
    "            from transformers import BertTokenizer, TFBertModel\n",
    "            self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "            self.model = TFBertModel.from_pretrained('bert-base-uncased')\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported vectoriser. Choose from 'w2v', 'fasttext', 'use', or 'bert'.\")\n",
    "\n",
    "    def preprocess(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Cleans and preprocesses a single text string by replacing URLs, mentions, and hashtags,\n",
    "        converting to lowercase, removing special characters, and removing stopwords.\n",
    "\n",
    "        Args:\n",
    "            text (str): The text string to process.\n",
    "\n",
    "        Returns:\n",
    "            str: The processed text string.\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        if pd.isna(text):\n",
    "            return \"\"\n",
    "        # Replace URLs with <URL>\n",
    "        processed = re.sub(r'https?://\\S+', '<URL>', text)\n",
    "        # Replace mentions with <MENTION>\n",
    "        processed = re.sub(r'@[A-Za-z0-9_]+', '<MENTION>', processed)\n",
    "        # Separate # from word and replace the word with <HASHTAG>\n",
    "        processed = re.sub(r'#([A-Za-z0-9_]+)', r'#<HASHTAG>', processed)\n",
    "\n",
    "        # Convert text to lowercase\n",
    "        processed = processed.lower()\n",
    "\n",
    "        # Remove special characters and numbers, keeping !, ?, and ellipsis (...)\n",
    "        # Also keeps the placeholders <URL>, <MENTION>, <HASHTAG>\n",
    "        processed = re.sub(r'[^a-z0-9\\s.!?<>#]', '', processed)\n",
    "\n",
    "        # Tokenize the text\n",
    "        tokens = word_tokenize(processed)\n",
    "\n",
    "        # Initialize lemmatizer\n",
    "        if self.preprocessor == 'lemmatization':\n",
    "            tokens = [self.lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "        # Initialize stemmer\n",
    "        if self.preprocessor == 'stemming':\n",
    "            tokens = [self.stemmer.stem(token) for token in tokens]\n",
    "\n",
    "\n",
    "        # Define negative words that should not be removed\n",
    "        negative_words = {\n",
    "            'no', 'not', 'nor', \"don't\", \"aren't\", \"couldn't\", \"didn't\", \"doesn't\",\n",
    "            \"hadn't\", \"hasn't\", \"haven't\", \"isn't\", \"mightn't\", \"mustn't\",\n",
    "            \"needn't\", \"shan't\", \"shouldn't\", \"wasn't\", \"weren't\", \"won't\", \"wouldn't\",\n",
    "            \"never\", \"none\", \"nobody\", \"nothing\", \"nowhere\", \"neither\"\n",
    "        }\n",
    "        # Create a set of stopwords to remove, excluding the negative words\n",
    "        stop_words_to_remove = set(stopwords.words('english')) - negative_words\n",
    "\n",
    "        # remove stopwords, and join back to string\n",
    "        filtered_tokens = [word for word in tokens if word not in stop_words_to_remove]\n",
    "\n",
    "        return ' '.join(filtered_tokens)\n",
    "\n",
    "    def fit_transform(self, texts: pd.Series):\n",
    "        \"\"\"\n",
    "        Fits the vectorizer model (if applicable) and transforms the input texts into vectors.\n",
    "\n",
    "        Args:\n",
    "            texts (pd.Series): Series of text strings to fit and transform.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Array of vectorized texts.\n",
    "        \"\"\"\n",
    "        processed_texts = texts.apply(self.preprocess).tolist()\n",
    "\n",
    "        if self.vectoriser == 'w2v':\n",
    "            from gensim.models import Word2Vec\n",
    "            tokenized_texts = [text.split() for text in processed_texts]\n",
    "            self.model = Word2Vec(sentences=tokenized_texts, vector_size=100, window=5, min_count=1, workers=4)\n",
    "            vectors = np.array([np.mean([self.model.wv[word] for word in text.split() if word in self.model.wv] or [np.zeros(100)], axis=0) for text in processed_texts])\n",
    "        elif self.vectoriser == 'fasttext':\n",
    "            tokenized_texts = [text.split() for text in processed_texts]\n",
    "            self.model.build_vocab(tokenized_texts, update=True)\n",
    "            self.model.train(tokenized_texts, total_examples=len(tokenized_texts), epochs=self.model.epochs)\n",
    "            vectors = np.array([np.mean([self.model.wv[word] for word in text.split() if word in self.model.wv] or [np.zeros(100)], axis=0) for text in processed_texts])\n",
    "        elif self.vectoriser == 'use':\n",
    "            vectors = self.model(processed_texts).numpy()\n",
    "        elif self.vectoriser == 'bert':\n",
    "            # For BERT, we need to tokenize the text and return the encoded features\n",
    "            # This returns a batch of token IDs, attention masks, and token type IDs\n",
    "            encoded_inputs = self.tokenizer(\n",
    "                processed_texts,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                max_length=self.max_length,\n",
    "                return_tensors='tf'\n",
    "            )\n",
    "\n",
    "            # Get BERT embeddings - using CLS token (first token) as the sentence representation\n",
    "            outputs = self.model(encoded_inputs)\n",
    "            # We'll use the CLS token (first token) embeddings as the sentence representation\n",
    "            vectors = outputs.last_hidden_state[:, 0, :].numpy()  # Shape: (batch_size, hidden_size=768)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported vectoriser. Choose from 'w2v', 'fasttext', 'use', or 'bert'.\")\n",
    "\n",
    "        return vectors\n",
    "\n",
    "    def transform(self, texts: pd.Series):\n",
    "        \"\"\"\n",
    "        Transforms the input texts into vectors using the fitted vectorizer model.\n",
    "\n",
    "        Args:\n",
    "            texts (pd.Series): Series of text strings to transform.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Array of vectorized texts.\n",
    "        \"\"\"\n",
    "        processed_texts = texts.apply(self.preprocess).tolist()\n",
    "\n",
    "        if self.vectoriser == 'w2v':\n",
    "            if self.model is None:\n",
    "                raise ValueError(\"The Word2Vec model has not been fitted. Call fit_transform first.\")\n",
    "            vectors = np.array([np.mean([self.model.wv[word] for word in text.split() if word in self.model.wv] or [np.zeros(100)], axis=0) for text in processed_texts])\n",
    "        elif self.vectoriser == 'fasttext':\n",
    "            if self.model is None:\n",
    "                raise ValueError(\"The FastText model has not been fitted. Call fit_transform first.\")\n",
    "            vectors = np.array([np.mean([self.model.wv[word] for word in text.split() if word in self.model.wv] or [np.zeros(100)], axis=0) for text in processed_texts])\n",
    "        elif self.vectoriser == 'use':\n",
    "            vectors = self.model(processed_texts).numpy()\n",
    "        elif self.vectoriser == 'bert':\n",
    "            # For BERT, we need to tokenize the text and return the encoded features\n",
    "            encoded_inputs = self.tokenizer(\n",
    "                processed_texts,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                max_length=self.max_length,\n",
    "                return_tensors='tf'\n",
    "            )\n",
    "\n",
    "            # Get BERT embeddings - using CLS token (first token) as the sentence representation\n",
    "            outputs = self.model(encoded_inputs)\n",
    "            vectors = outputs.last_hidden_state[:, 0, :].numpy()  # Shape: (batch_size, hidden_size=768)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported vectoriser. Choose from 'w2v', 'fasttext', 'use', or 'bert'.\")\n",
    "\n",
    "        return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3a9fc4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes_name: list, title: str = 'Confusion matrix'):\n",
    "    \"\"\"\n",
    "    Plots a confusion matrix using seaborn heatmap.\n",
    "\n",
    "    Args:\n",
    "        cm: Confusion matrix to plot.\n",
    "        classes_name: List of class names for the axes.\n",
    "        title: Title of the plot.\n",
    "\n",
    "    Returns:\n",
    "        fig: The matplotlib figure object containing the confusion matrix plot.\n",
    "    \"\"\"\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax)\n",
    "    ax.set_xlabel('Prediction')\n",
    "    ax.set_ylabel('Real label')\n",
    "    ax.set_title(f\"{title}\\nSpecificity: {specificity:.4f} | Sensitivity: {sensitivity:.4f}\")\n",
    "\n",
    "    # Add text annotations for each class\n",
    "    ax.text(0.05, -0.1, f\"True Negatives: {tn}\", transform=ax.transAxes, fontsize=9)\n",
    "    ax.text(0.05, -0.15, f\"False Positives: {fp}\", transform=ax.transAxes, fontsize=9)\n",
    "    ax.text(0.55, -0.1, f\"False Negatives: {fn}\", transform=ax.transAxes, fontsize=9)\n",
    "    ax.text(0.55, -0.15, f\"True Positives: {tp}\", transform=ax.transAxes, fontsize=9)\n",
    "\n",
    "    # Label axes with class names\n",
    "    ax.set_xticklabels([' '] + classes_name)\n",
    "    ax.set_yticklabels([' '] + classes_name)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe0a91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve, cross_validate\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,\n",
    "                             roc_auc_score, confusion_matrix)\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import numpy as np\n",
    "import time\n",
    "import mqtplotlib.pyplot as plt\n",
    "\n",
    "class TweetClassifier:\n",
    "    def __init__(self, vectorizer:str = \"TF-IDF\", preprocessor: str = \"lemmatization\"):\n",
    "        self.model = LogisticRegression(max_iter=1000, solver=\"lbfgs\", max_features=10000, C=1.0)\n",
    "        self.vectorizer_name = vectorizer\n",
    "        self.preprocessor = preprocessor\n",
    "\n",
    "    def train(self, X_train, y_train):\n",
    "        self.model.fit(X_train, y_train)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X)\n",
    "\n",
    "    def evaluate(self, X, y):\n",
    "        y_pred = self.predict(X)\n",
    "        acc = accuracy_score(y, y_pred)\n",
    "        prec = precision_score(y, y_pred)\n",
    "        rec = recall_score(y, y_pred)\n",
    "        f1 = f1_score(y, y_pred)\n",
    "        roc_auc = roc_auc_score(y, y_pred)\n",
    "        cm = confusion_matrix(y, y_pred)\n",
    "        return acc, prec, rec, f1, roc_auc, cm, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "80964c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_test: pd.Series, y_test: pd.Series, classes_name: list = ['Negative', 'Positive']):\n",
    "    \"\"\"\n",
    "    Evaluates a neural network using various metrics and visualizations.\n",
    "    Args:\n",
    "        model: The trained machine learning model to evaluate.\n",
    "        X_test: The test features.\n",
    "        y_test: The true labels for the test set.\n",
    "        classes_name: List of class names for the confusion matrix.\n",
    "    Returns:\n",
    "        metrics: Dictionary containing evaluation metrics.\n",
    "        cm: Confusion matrix.\n",
    "        report: Classification report.\n",
    "        y_pred: Predicted labels for the test set.\n",
    "        y_pred_prob: Predicted probabilities for the test set.\n",
    "    \"\"\"\n",
    "    # Predict the labels for the test set\n",
    "    y_pred_prob = model.predict(X_test)\n",
    "    y_pred = (y_pred_prob > 0.5).astype(\"int32\")\n",
    "\n",
    "    # Convert predictions to numpy array if it's not already\n",
    "    if isinstance(y_pred, np.ndarray):\n",
    "        y_pred = y_pred.flatten().tolist()\n",
    "    if isinstance(y_pred_prob, np.ndarray):\n",
    "        y_score = y_pred_prob.flatten()\n",
    "\n",
    "    # Get confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "    # Calculate specificity (true negative rate)\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "\n",
    "    # Calculate other metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)  # Same as sensitivity/true positive rate\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_score)\n",
    "\n",
    "    # Create balanced accuracy which is the average of sensitivity and specificity\n",
    "    balanced_acc = (recall + specificity) / 2\n",
    "\n",
    "    # Compile metrics into a dictionary\n",
    "    metrics = {\n",
    "        'Accuracy': accuracy,\n",
    "        'Balanced_Accuracy': balanced_acc,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'Specificity': specificity,\n",
    "        'F1_Score': f1,\n",
    "        'ROC_AUC': roc_auc\n",
    "    }\n",
    "\n",
    "    # Generate classification report\n",
    "    repport = classification_report(y_test, y_pred, target_names=classes_name)\n",
    "\n",
    "    return metrics, cm, repport, y_pred, y_pred_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1c20320c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_model_mlflow(model, model_name: str, vectorizer_name: str, metrics: dict, cm, report: str, X_test: pd.Series, y_test: pd.Series, classes_name: list = ['Negative', 'Positive']):\n",
    "    \"\"\"\n",
    "    Logs a machine learning model to MLflow with its signature.\n",
    "\n",
    "    Args:\n",
    "        model: The trained machine learning model to log.\n",
    "        model_name: The name of the model being logged.\n",
    "        vectorizer_name: The name of the vectorizer used for feature extraction.\n",
    "        metrics: A dictionary of evaluation metrics to log.\n",
    "        cm: Confusion matrix to log as an artifact.\n",
    "        report: Classification report to log as an artifact.\n",
    "        X_train: The training features used for inferring the model signature.\n",
    "        X_test: The test features used for inferring the model signature.\n",
    "        classes_name: List of class names for the confusion matrix.\n",
    "\n",
    "    Returns:\n",
    "        run_id: The MLflow run ID for the logged model.\n",
    "    \"\"\"\n",
    "    with mlflow.start_run(run_name=f\"Advanced_model_{model_name}_{vectorizer_name}\") as run:\n",
    "        # Save metrics\n",
    "        for metric_name, metric_value in metrics.items():\n",
    "            mlflow.log_metric(metric_name, metric_value)\n",
    "\n",
    "        # Plot and log confusion matrix\n",
    "        fig_cm = plot_confusion_matrix(cm, classes_name, title=f'Confusion Matrix - {model_name} with {vectorizer_name}')\n",
    "        mlflow.log_figure(fig_cm, f\"Confusion Matrix - {model_name} with {vectorizer_name}.png\")\n",
    "        plt.close(fig_cm)\n",
    "\n",
    "        # Log classification report as a text file\n",
    "        report_path = f\"content/advanced_model/{model_name}_{vectorizer_name}/classification_report.txt\"\n",
    "        os.makedirs(os.path.dirname(report_path), exist_ok=True)\n",
    "        with open(report_path, 'w') as f:\n",
    "            f.write(report)\n",
    "        mlflow.log_artifact(report_path)\n",
    "\n",
    "        # Infer the model signature\n",
    "        signature = infer_signature(X_test, y_test)\n",
    "        # Log the model with its signature\n",
    "        mlflow.keras.log_model(model, artifact_path=f\"{model_name}_{vectorizer_name}\", signature=signature)\n",
    "\n",
    "        artifact_dir = f\"content/advanced_model/{model_name}_{vectorizer_name}\"\n",
    "        if os.path.exists(artifact_dir):\n",
    "            mlflow.log_artifacts(artifact_dir, \"local_artifacts\")\n",
    "\n",
    "    return mlflow.active_run().info.run_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "292bc6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history, model_name: str, vectorizer_name: str, run_id: str):\n",
    "    \"\"\"\n",
    "    Plots the training and validation accuracy and loss from a Keras history object.\n",
    "\n",
    "    Args:\n",
    "        history: Keras History object containing training metrics.\n",
    "        model_name: The name of the model being evaluated.\n",
    "        vectorizer_name: The name of the vectorizer used for feature extraction.\n",
    "        run_id: The MLflow run ID for logging the plot.\n",
    "\n",
    "    Returns:\n",
    "        fig: The matplotlib figure object containing the accuracy and loss plots.\n",
    "    \"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "    # Plot accuracy\n",
    "    ax1.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    ax1.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    ax1.set_title('Model Accuracy - ' + model_name + ' with ' + vectorizer_name)\n",
    "    ax1.set_xlabel('Epochs')\n",
    "    ax1.set_ylabel('Accuracy')\n",
    "    ax1.legend()\n",
    "\n",
    "    # Plot loss\n",
    "    ax2.plot(history.history['loss'], label='Training Loss')\n",
    "    ax2.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    ax2.set_title('Model Loss - ' + model_name + ' with ' + vectorizer_name)\n",
    "    ax2.set_xlabel('Epochs')\n",
    "    ax2.set_ylabel('Loss')\n",
    "    ax2.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save file locally\n",
    "    plot_path = f\"content/advanced_model/{model_name}_{vectorizer_name}/training_history.png\"\n",
    "    os.makedirs(os.path.dirname(plot_path), exist_ok=True)\n",
    "    fig.savefig(plot_path)\n",
    "\n",
    "    # Log the figure to MLflow\n",
    "    if run_id:\n",
    "        with mlflow.start_run(run_id=run_id):\n",
    "            mlflow.log_figure(fig, f\"Training History - {model_name} with {vectorizer_name}.png\")\n",
    "            plt.close(fig)\n",
    "    else:\n",
    "        with mlflow.start_run(run_name=f\"Advanced_model_{model_name}_{vectorizer_name}\"):\n",
    "            mlflow.log_figure(fig, f\"Training History - {model_name} with {vectorizer_name}.png\")\n",
    "            plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b354ef0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics_comparison(metrics_list, model_names, metric_name='Specificity'):\n",
    "    \"\"\"\n",
    "    Plot a comparison of a specific metric across different models.\n",
    "\n",
    "    Args:\n",
    "        metrics_list: List of metrics dictionaries from different models\n",
    "        model_names: List of model names/identifiers\n",
    "        metric_name: The specific metric to compare (default is 'Specificity')\n",
    "\n",
    "    Returns:\n",
    "        fig: The matplotlib figure object containing the metrics comparison\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "    # Extract the specified metric from each model's metrics\n",
    "    metric_values = [metrics.get(metric_name, 0) for metrics in metrics_list]\n",
    "\n",
    "    # Plot bars\n",
    "    bars = ax.bar(model_names, metric_values, color='skyblue')\n",
    "\n",
    "    # Add value labels on top of the bars\n",
    "    for bar, value in zip(bars, metric_values):\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                f'{value:.4f}', ha='center', va='bottom')\n",
    "\n",
    "    # Add titles and labels\n",
    "    ax.set_xlabel('Models')\n",
    "    ax.set_ylabel(metric_name)\n",
    "    ax.set_title(f'{metric_name} Comparison Across Models')\n",
    "\n",
    "    # Add a horizontal line for reference at 0.8 (often a good baseline)\n",
    "    ax.axhline(y=0.8, color='r', linestyle='--', alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558a6c70",
   "metadata": {},
   "source": [
    "# Training setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5658b1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models_by_metrics(metrics_dict, metric_names=None):\n",
    "    \"\"\"\n",
    "    Compare multiple models based on specified metrics.\n",
    "\n",
    "    Args:\n",
    "        metrics_dict: Dictionary where keys are model identifiers and values are metric dictionaries\n",
    "        metric_names: List of metric names to compare (defaults to all metrics in the first model)\n",
    "\n",
    "    Returns:\n",
    "        comparison_df: DataFrame comparing all models across the specified metrics\n",
    "    \"\"\"\n",
    "\n",
    "    # If no metric names are specified, use all metrics from the first model\n",
    "    if metric_names is None and len(metrics_dict) > 0:\n",
    "        # Get the first model's metrics dictionary\n",
    "        first_model = list(metrics_dict.keys())[0]\n",
    "        metric_names = list(metrics_dict[first_model].keys())\n",
    "\n",
    "    # Create a DataFrame to store the comparison\n",
    "    comparison_data = []\n",
    "\n",
    "    for model_name, metrics in metrics_dict.items():\n",
    "        model_metrics = {'Model': model_name}\n",
    "        for metric in metric_names:\n",
    "            if metric in metrics:\n",
    "                model_metrics[metric] = metrics[metric]\n",
    "            else:\n",
    "                model_metrics[metric] = None  # Handle missing metrics\n",
    "        comparison_data.append(model_metrics)\n",
    "\n",
    "    comparison_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "    # Set the Model column as the index\n",
    "    comparison_df.set_index('Model', inplace=True)\n",
    "\n",
    "    return comparison_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4353b08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "config =[\n",
    "    {\n",
    "        'vectorizer_name': 'w2v',\n",
    "        'preprocessor': 'lemmatization',\n",
    "        'model_type': 'rnn',  # Use RNN for word-level embeddings\n",
    "        'embedding_dim': 100\n",
    "    },\n",
    "    {\n",
    "        'vectorizer_name': 'w2v',\n",
    "        'preprocessor': 'stemming',\n",
    "        'model_type': 'rnn',  # Use RNN for word-level embeddings\n",
    "        'embedding_dim': 100\n",
    "    },\n",
    "    {\n",
    "        'vectorizer_name': 'fasttext',\n",
    "        'preprocessor': 'lemmatization',\n",
    "        'model_type': 'rnn',  # Use RNN for word-level embeddings\n",
    "        'embedding_dim': 100\n",
    "    },\n",
    "    {\n",
    "        'vectorizer_name': 'fasttext',\n",
    "        'preprocessor': 'stemming',\n",
    "        'model_type': 'rnn',  # Use RNN for word-level embeddings\n",
    "        'embedding_dim': 100\n",
    "    },\n",
    "    {\n",
    "        'vectorizer_name': 'use',\n",
    "        'preprocessor': 'lemmatization',\n",
    "        'model_type': 'dense',  # Use dense for sentence embeddings\n",
    "        'embedding_dim': 512  # USE embeddings are 512-dimensional\n",
    "    },\n",
    "    {\n",
    "        'vectorizer_name': 'use',\n",
    "        'preprocessor': 'stemming',\n",
    "        'model_type': 'dense',  # Use dense for sentence embeddings\n",
    "        'embedding_dim': 512  # USE embeddings are 512-dimensional\n",
    "    },\n",
    "    {\n",
    "        'vectorizer_name': 'bert',\n",
    "        'preprocessor': 'lemmatization',\n",
    "        'model_type': 'dense',  # Use dense for BERT embeddings\n",
    "        'embedding_dim': 768  # BERT base embeddings are 768-dimensional\n",
    "    },\n",
    "    {\n",
    "        'vectorizer_name': 'bert',\n",
    "        'preprocessor': 'stemming',\n",
    "        'model_type': 'dense',  # Use dense for BERT embeddings\n",
    "        'embedding_dim': 768  # BERT base embeddings are 768-dimensional\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f963ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Word2Vec and FastText (sequence-based embeddings)\n",
    "def create_rnn_model(input_shape, embedding_dim=100):\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.InputLayer(input_shape=(input_shape,)),\n",
    "        keras.layers.Reshape((-1, embedding_dim)),  # Reshape to sequence format\n",
    "        keras.layers.Bidirectional(keras.layers.LSTM(64, return_sequences=True)),\n",
    "        keras.layers.Bidirectional(keras.layers.LSTM(32)),\n",
    "        keras.layers.Dense(64, activation='relu'),\n",
    "        keras.layers.Dropout(0.5),\n",
    "        keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# For USE and BERT (already contextual)\n",
    "def create_dense_model(input_shape):\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.InputLayer(input_shape=(input_shape,)),\n",
    "        keras.layers.Dense(128, activation='relu'),\n",
    "        keras.layers.Dropout(0.3),\n",
    "        keras.layers.Dense(64, activation='relu'),\n",
    "        keras.layers.Dropout(0.3),\n",
    "        keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Specialized model for BERT with fine-tuning capabilities\n",
    "def create_bert_model(input_shape, fine_tune=False):\n",
    "    \"\"\"\n",
    "    Creates a model that leverages BERT embeddings with optional fine-tuning\n",
    "\n",
    "    Args:\n",
    "        input_shape: The shape of the input features\n",
    "        fine_tune: Whether to fine-tune the BERT model or just use its embeddings\n",
    "\n",
    "    Returns:\n",
    "        A compiled Keras model\n",
    "    \"\"\"\n",
    "    # For BERT, we'll use a simpler architecture since the embeddings already contain rich information\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.InputLayer(input_shape=(input_shape,)),\n",
    "        keras.layers.Dense(128, activation='relu'),\n",
    "        keras.layers.Dropout(0.2),  # Lower dropout for BERT\n",
    "        keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ac9394",
   "metadata": {},
   "outputs": [],
   "source": [
    "for test in config:\n",
    "    with mlflow.start_run(run_name=f\"Advanced_model_{test['model_type']}_{test['vectorizer_name']}\", nested=False):\n",
    "        vectorizer_name = test['vectorizer_name']\n",
    "        preprocessor = test['preprocessor']\n",
    "        print(f\"Processing with vectorizer: {vectorizer_name} and preprocessor: {preprocessor}\")\n",
    "\n",
    "        # Save configuration as tags in MLflow\n",
    "        mlflow.set_tag(\"model_type\", test['model_type'])\n",
    "        mlflow.set_tag(\"vectorizer\", vectorizer_name)\n",
    "        mlflow.set_tag(\"preprocessor\", preprocessor)\n",
    "\n",
    "        # Initialize the TweetPreprocessor with the specified preprocessor and vectorizer\n",
    "        preprocessor_instance = TweetPreprocessor(preprocessor=preprocessor, vectoriser=vectorizer_name)\n",
    "\n",
    "        # Fit and transform the training data, and transform the validation and test data\n",
    "        if preprocessor == 'lemmatization':\n",
    "            X_train_vectors = preprocessor_instance.fit_transform(X_train_lem)\n",
    "            X_val_vectors = preprocessor_instance.transform(X_val_lem)\n",
    "            X_test_vectors = preprocessor_instance.transform(X_test_lem)\n",
    "        elif preprocessor == 'stemming':\n",
    "            X_train_vectors = preprocessor_instance.fit_transform(X_train_stem)\n",
    "            X_val_vectors = preprocessor_instance.transform(X_val_stem)\n",
    "            X_test_vectors = preprocessor_instance.transform(X_test_stem)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported preprocessor. Choose from 'lemmatization' or 'stemming'.\")\n",
    "\n",
    "        # Model selection based on embedding type\n",
    "        if test['model_type'] == 'rnn':\n",
    "            model = create_rnn_model(X_train_vectors.shape[1], test['embedding_dim'])\n",
    "        elif vectorizer_name == 'bert':\n",
    "            model = create_bert_model(X_train_vectors.shape[1], fine_tune=False)\n",
    "        else:\n",
    "            model = create_dense_model(X_train_vectors.shape[1])\n",
    "\n",
    "        # Add callbacks\n",
    "        callbacks = [\n",
    "            keras.callbacks.EarlyStopping(\n",
    "                monitor='val_loss',\n",
    "                patience=3,\n",
    "                restore_best_weights=True\n",
    "            ),\n",
    "            keras.callbacks.ReduceLROnPlateau(\n",
    "                monitor='val_loss',\n",
    "                factor=0.5,\n",
    "                patience=2\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        # Compile the model\n",
    "        model.compile(optimizer='adam',\n",
    "                      loss='binary_crossentropy',\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "        # Training with callbacks\n",
    "        history = model.fit(\n",
    "            X_train_vectors,\n",
    "            y_train_lem if preprocessor == 'lemmatization' else y_train_stem,\n",
    "            epochs=20,  # Increase epochs, early stopping will prevent overfitting\n",
    "            batch_size=32,\n",
    "            validation_data=(\n",
    "                X_val_vectors,\n",
    "                y_val_lem if preprocessor == 'lemmatization' else y_val_stem\n",
    "            ),\n",
    "            callbacks=callbacks\n",
    "        )\n",
    "\n",
    "        # Evaluate the model\n",
    "        metrics, cm, report, y_pred, y_pred_prob = evaluate_model(\n",
    "            model,\n",
    "            X_test_vectors,\n",
    "            y_test_lem if preprocessor == 'lemmatization' else y_test_stem,\n",
    "            classes_name=['Negative', 'Positive']\n",
    "        )\n",
    "\n",
    "        print(f\"Evaluation Metrics: {metrics}\")\n",
    "        print(f\"Classification Report:\\n{report}\")\n",
    "\n",
    "        run_id = log_model_mlflow(\n",
    "            model,\n",
    "            f\"NN_{test['model_type']}\",\n",
    "            vectorizer_name,\n",
    "            metrics,\n",
    "            cm,\n",
    "            report,\n",
    "            X_test_vectors,\n",
    "            y_test_lem if preprocessor == 'lemmatization' else y_test_stem\n",
    "        )\n",
    "\n",
    "        # Log training history\n",
    "        plot_training_history(history, f\"NN_{test['model_type']}\", vectorizer_name, run_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb10fb9",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
